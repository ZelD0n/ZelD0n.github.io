<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/01/Unsupervised%20Continual%20Domain%20Shift%20Learning%20with%20Multi-Prototype%20Modeling/" class="article-date">
  <time class="dt-published" datetime="2025-07-01T09:15:01.993Z" itemprop="datePublished">2025-07-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/01/Unsupervised%20Continual%20Domain%20Shift%20Learning%20with%20Multi-Prototype%20Modeling/">Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling_CVPR_2025_paper.pdf">https://openaccess.thecvf.com/content/CVPR2025/papers/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling_CVPR_2025_paper.pdf</a></strong><br><strong>Code URL: TBD</strong></p>
<p><strong>Conference: CVPR’25</strong></p>
</blockquote>
<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><p>To overcome the adaptivity gap issue in Unsupervised Continual Domain Shift Learning, the paper proposes <strong>Multi-Prototype Modeling</strong> (<strong>MPM</strong>) , a method comprises two main parts: <strong>Multi-Prototype Learning</strong> (<strong>MPL</strong>) and <strong>Bi-Level Graph Enhancer</strong> (<strong>BiGE</strong>).</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Unsupervised-Continual-Domain-Shift-Learning"><a href="#Unsupervised-Continual-Domain-Shift-Learning" class="headerlink" title="Unsupervised Continual Domain Shift Learning"></a>Unsupervised Continual Domain Shift Learning</h2><p><strong>Unsupervised Continual Domain Shift Learning</strong> (<strong>UCDSL</strong>) aims to adapt a pre-trained model to dynamically shifting target domains without access to labeled data from either the source or target domains.<br>Existing methods attempt to learn a universal classifier for all domains by enforcing feature alignment across different domains. However, this assumption is not guaranteed and the classifier may deviate significantly from the optimal classifier for a specific target domain.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/01/Unsupervised%20Continual%20Domain%20Shift%20Learning%20with%20Multi-Prototype%20Modeling/" data-id="cmclkr40l00046cfy13yf1un8" data-title="Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine-learning</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Contrastive Sparse Representation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/06/30/Contrastive%20Sparse%20Representation/" class="article-date">
  <time class="dt-published" datetime="2025-06-30T12:29:51.164Z" itemprop="datePublished">2025-06-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/06/30/Contrastive%20Sparse%20Representation/">Contrastive Sparse Representation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.01776">https://arxiv.org/pdf/2503.01776</a></strong><br><strong>Code URL: <a target="_blank" rel="noopener" href="https://github.com/neilwen987/CSR_Adaptive_Rep">https://github.com/neilwen987/CSR_Adaptive_Rep</a></strong></p>
<p><strong>Conference: ICML’25 Oral</strong></p>
</blockquote>
<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><p>This paper introduces <strong>CSR</strong>, an effective learning method for sparse adaptive representations. It combines a task-specific sparse contrastive learning loss with a reconstructive loss to maintain overall embedding quality.</p>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.13147"><strong>MRL</strong></a>, as an existing method to compress representations, truncates the linear layers into a set of target sizes to reflect representations of various dimensions.<br><img src="/images/MRL.png" alt="MRL.png"><br>However, MRL faces two key constraints:</p>
<ol>
<li>it requires (full) training of the backbone parameters;</li>
<li>its performance often deteriorates a lot under small hidden dimensions.<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><strong>CSR</strong> combines <strong><em>sparse contrastive learning</em></strong> and <strong><em>sparse autoencoding</em></strong> to maintain overall embedding quality.<br><img src="/images/CSR.png" alt="CSR.png"><h2 id="Sparse-Autoencoders-SAEs"><a href="#Sparse-Autoencoders-SAEs" class="headerlink" title="Sparse Autoencoders(SAEs)"></a>Sparse Autoencoders(SAEs)</h2><strong>SAEs</strong> aim to extract a sparse representation $z_k$ by learning to reconstruct the dense feature from $z_k$.</li>
</ol>
<p>Denote the (weights, bias) of encoder and decoder as $(W_{enc},b_{enc})$ and $(W_{dec},b_{pre})$. And $f(x)$ represents the dense feature of input $x$.</p>
<h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><script type="math/tex; mode=display">
    z_k:=\sigma^+(\text{TopK}(W_{enc}(f(x)-b_{pre})+b_{enc}))</script><p>$\text{TopK}$ gets $K$ largest values. $\sigma^+(\cdot)=max(0,\cdot)$ is the ReLU activation.<br>This process encodes the dense representations into sparse representations, with only $K$ non-zero values.</p>
<h3 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h3><script type="math/tex; mode=display">
    \widehat{f(x)_k}:=W_{dec}z_k+b_{pre}</script><p>This process decodes and reconstructs the original embedding using sparse representation $z_k$. To make $\widehat{f(x)_k}$ as similar to $f(x)$ as possible, it uses L2 reconstruction loss as the main training objective:</p>
<script type="math/tex; mode=display">
    \mathcal{L}(k)=||f(x) - \widehat{f(x)_k}||_2^2.</script><h3 id="Dead-Latents"><a href="#Dead-Latents" class="headerlink" title="Dead Latents"></a><em>Dead Latents</em></h3><p>As the large number of zeros in sparse representations, a lot of latent dimensions remain inactive during training - a phenomenon called the <strong><em>Dead Latents</em></strong>. This is one of the reasons of the performance degradation.<br>To mitigate this issue, an auxiliary loss $\mathcal{L}_{aux}$ and Multi-TopK losses are proposed.</p>
<h4 id="Auxiliary-Loss"><a href="#Auxiliary-Loss" class="headerlink" title="Auxiliary Loss"></a>Auxiliary Loss</h4><script type="math/tex; mode=display">
    \mathcal{L}_{aux} = ||e-\hat{e}||_2^2,</script><p>where $e=f(x)-\widehat{f(x)}$, and $\hat{e}=W_{dec}z$. This is the forced reconstruction using the top-$k_{aux}$ dead latents.</p>
<h4 id="Multi-TopK-Losses"><a href="#Multi-TopK-Losses" class="headerlink" title="Multi-TopK Losses"></a>Multi-TopK Losses</h4><p>An extra $4k$ reconstruction is added.</p>
<p>The overall construction loss is</p>
<script type="math/tex; mode=display">
    \mathcal{L}_{recon}=\mathcal{L}(k)+\frac{1}{8}\mathcal{L}(4k)+\beta\mathcal{L}_{aux}.</script><h2 id="Sparse-Contrastive-Learning"><a href="#Sparse-Contrastive-Learning" class="headerlink" title="Sparse Contrastive Learning"></a>Sparse Contrastive Learning</h2><p>Simply reconstruction is not enough to make sparse representations distinguishable. So contrastive learning is introduced to enhance the semantic discriminative power of sparse representations.</p>
<p>Supposing batch size $\mathcal{B}$. $z_i$ represents the $i$-th sample in the batch. The contrastive loss is defined as</p>
<script type="math/tex; mode=display">
    \mathcal{L}_{cl}=-\frac{1}{\mathcal{B}}\sum^{\mathcal{B}}_{i=1}\log{\frac{\exp(z_i^Tz_i)}{\exp(z_i^Tz_i)+\sum_{j\ne i}^{\mathcal{B}}\exp(z_i^Tz_j)}}</script><p>This makes each latent representations closer to the positive samples (itself) and further to the negative samples (other representations).<br>Notice that all the $z$ are non-negative by $\text{ReLU} + \text{TopK}$ selection, thus this contrastive loss is a variant of the Non-negative Contrastive Loss (NCL).</p>
<blockquote>
<p><strong><em>Theorem 5.</em></strong> <em>Under mild conditions, the solution $\phi(x)$ is the unique solution to the NCL objective. As a result, NCL features are identifiable and disentangled.</em></p>
</blockquote>
<p>According to the above theorem, the sparse contrastive loss tends to relate each latent dimension to a specific stable and distinguishable semantic factor. It also helps mitigate the dead latents issue.</p>
<h2 id="Overall-Training-Objectiveness"><a href="#Overall-Training-Objectiveness" class="headerlink" title="Overall Training Objectiveness"></a>Overall Training Objectiveness</h2><script type="math/tex; mode=display">
    \mathcal{L}_{CSR}=\mathcal{L}_{recon}+\gamma\mathcal{L}_{ncl}.</script>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/06/30/Contrastive%20Sparse%20Representation/" data-id="cmclkr40k00016cfycq9idlnl" data-title="Contrastive Sparse Representation" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine-learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vision-language-model/" rel="tag">vision-language-model</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/06/23/Adversarial%20Pre-training%20and%20Instruction%20Tuning%20for%20Improving%20Vision-Language%20Model%20Robustness/" class="article-date">
  <time class="dt-published" datetime="2025-06-23T09:07:24.991Z" itemprop="datePublished">2025-06-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/06/23/Adversarial%20Pre-training%20and%20Instruction%20Tuning%20for%20Improving%20Vision-Language%20Model%20Robustness/">Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.09446">https://arxiv.org/pdf/2501.09446</a></strong><br><strong>Code URL: <a target="_blank" rel="noopener" href="https://doublevisualdefense.github.io/">https://doublevisualdefense.github.io/</a></strong></p>
</blockquote>
<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><p>This paper aims to enhance the robustness of vision-language models against adversarial visual perturbations. It proposes <strong><em>Double Visual Defense</em></strong>, a large-scale adversarial vision-language pre-training method. This method contains two stages: <strong><em>Adversarial Contrastive Pre-Training</em></strong> and <strong><em>Adversarial Visual Instruction-Tuning</em></strong>. In experiments, it showcases robustness improvement, stronger zero-shot recognition capability, fewer hallucinations and superior reasoning performance.</p>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>Traditional adversarial robustness methods focus more on post-hoc fine-tuning. Double Visual Defense, in contrast, propose to both CLIP pre-training and visual instruction tuning.</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="\images\Pasted image 20250623172002.png" alt="1"></p>
<h2 id="Adversarial-Contrastive-Pre-Training"><a href="#Adversarial-Contrastive-Pre-Training" class="headerlink" title="Adversarial Contrastive Pre-Training"></a>Adversarial Contrastive Pre-Training</h2><p>∆CLIP is trained to predict the right image-text pairings given adversarial images that are optimized to fool the model into predicting incorrect image-text pairings.</p>
<h2 id="Adversarial-Visual-Instruction-Tuning"><a href="#Adversarial-Visual-Instruction-Tuning" class="headerlink" title="Adversarial Visual Instruction-Tuning"></a>Adversarial Visual Instruction-Tuning</h2><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ol>
<li>Is adversarial robustness the same thing as domain robust? I’ve done research on test-time prompt tuning to enhance zero-shot and cross-domain generalization of vision-language models. Are they the same? If not, what‘s the difference between cross-domain, out-of-distribution and adversarial robustness?</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/06/23/Adversarial%20Pre-training%20and%20Instruction%20Tuning%20for%20Improving%20Vision-Language%20Model%20Robustness/" data-id="cmclkr40i00006cfy5engc0br" data-title="Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vision-language-model/" rel="tag">vision-language-model</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-OOD Detection" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/06/21/OOD%20Detection/" class="article-date">
  <time class="dt-published" datetime="2025-06-21T08:31:46.713Z" itemprop="datePublished">2025-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/06/21/OOD%20Detection/">OOD Detection</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Original: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102870562">https://zhuanlan.zhihu.com/p/102870562</a></strong></p>
<h1 id="What-is-OOD-Detection"><a href="#What-is-OOD-Detection" class="headerlink" title="What is OOD Detection?"></a>What is OOD Detection?</h1></blockquote>
<p>OOD(out-of-distribution) detection aims to detect an OOD sample, which is the opposite of ID(in-distribution) sample.</p>
<p>In traditional machine learning methods, the training and test data are assumed to be <strong>independent identical distribution(IID)</strong>. However, in real-world scenario, test samples are likely to be OOD, or outlier. Traditional deep learning models often consider an OOD sample as a class of ID samples with high confidence, which is not reasonable. Thus it is meaningful for models to recognize OOD samples, especially for AI security areas.</p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>Existing OOD Detection methods can be roughly divided into 4 categories:</p>
<ul>
<li>Softmax-based methods.</li>
<li>Uncertainty methods.</li>
<li>Generative model methods.</li>
<li>Classifier methods.<h2 id="Softmax-based-Methods"><a href="#Softmax-based-Methods" class="headerlink" title="Softmax-based Methods"></a>Softmax-based Methods</h2>Softmax-based methods get the softmax values of the outputs of a pre-trained model. And find out how OOD and ID samples distribute by statistical analysis. It aims to enlarge the difference between OOD and ID distributions.</li>
</ul>
<p>Softmax-based methods are simple, entirely training-free yet effective, without the need to change model structure.</p>
<h3 id="SMOOD"><a href="#SMOOD" class="headerlink" title="SMOOD"></a>SMOOD</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02136">https://arxiv.org/abs/1610.02136</a></strong></p>
</blockquote>
<p><strong>SMOOD</strong>(<strong>S</strong>oft<strong>M</strong>ax <strong>OOD</strong>) is the very first work of OOD Detection task. It proposes an OOD baseline method. Its main insights are:</p>
<ol>
<li>Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection.</li>
<li>Proposed a simple yet effective baseline method to detect whether a sample is mis-classified or out-of-distribution.</li>
</ol>
<p><img src="\images\Pasted image 20250621194118.png" alt="1"><br>The above tables show that using simply max softmax probabilities to in- or out-of-distribution samples is effective. But it can not tell if the model wrongly classifies the sample.</p>
<p><strong><em>In conclusion, SMOOD justifies the softmax output of the model differs when an in- or out-of-distribution sample is given. So they can be effectively discriminated by choosing a proper threshold.</em></strong></p>
<h3 id="ODIN"><a href="#ODIN" class="headerlink" title="ODIN"></a>ODIN</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02690">https://arxiv.org/abs/1706.02690</a></strong></p>
</blockquote>
<p>SMOOD is simple and effective. But the effect is not good enough. To achieve better results, <strong>ODIN</strong> proposes to enlarge the gap between in- and out-of-distribution softmax probabilities.</p>
<p>Based on above thoughts, it introduces two main methods:</p>
<ul>
<li>Temperature Scaling</li>
<li>Input Preprocessing</li>
</ul>
<h4 id="Temperature-Scaling"><a href="#Temperature-Scaling" class="headerlink" title="Temperature Scaling"></a>Temperature Scaling</h4><script type="math/tex; mode=display">
    p_i(x;T)=\frac{\exp(f_i(x)/T)}{\sum_{j=1}^N\exp(f_i(x)/T)}</script><p>A $T$ that is large enough makes the softmax probabilities close enough to $\frac{1}{N}$.</p>
<h4 id="Image-Preprocessing"><a href="#Image-Preprocessing" class="headerlink" title="Image Preprocessing"></a>Image Preprocessing</h4><p>ODIN preprocesss the input by adding small perturbations:</p>
<script type="math/tex; mode=display">
    \widetilde{x}=x-\epsilon\text{sign}(-\bigtriangledown_x\log(p_{\hat{y}}(x;T))</script><p>$x$ and $\epsilon$ denote the input sample and perturbation magnitude, respectively.<br>In adversarial examples(where $\widetilde{x}=x+\epsilon\text{sign}$), small perturbations are added to decrease the softmax score for the true label and force the model to make a wrong prediction. In ODIN, the goal is the opposite: it aims to increase the softmax score of any given input.<br><strong><em>Why it works?</em></strong> ID sample confidence are increased dramatically while OOD sample confidence stays the same. Thus the confidence gap between ID and OOD samples is enlarged.</p>
<h2 id="Uncertainty-Methods"><a href="#Uncertainty-Methods" class="headerlink" title="Uncertainty Methods"></a>Uncertainty Methods</h2><p>Uncertainty-based methods mainly learns the uncertainty of the model predictions. The uncertainty should be high given an OOD sample and low given an ID sample.</p>
<h3 id="Learning-Confidence-for-OOD-Detection"><a href="#Learning-Confidence-for-OOD-Detection" class="headerlink" title="Learning Confidence for OOD Detection"></a>Learning Confidence for OOD Detection</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.04865">https://arxiv.org/abs/1802.04865</a></strong></p>
</blockquote>
<p><strong><em>Motivation.</em></strong> In this paper the authors propose an idea that a network can estimate its prediction confidence and ask for “hints”.</p>
<p><img src="\images\Pasted image 20250621231213.png" alt="2"><br>In order to estimate the confidence of the prediction, a confidence estimation branch is added in parallel with the original class prediction branch, as shown in the above figure. It receives the same input as the class prediction branch and outputs the confidence.<br>The confidence branch contains one or more fully-connected layers, with the final layer outputting a single scalar $c$ between 0 and 1 (parameterized as a sigmoid).<br>The output of the prediction and confidence branch would be</p>
<script type="math/tex; mode=display">p, c = f(x, \Theta), p_i,c\in[0,1],\sum_{i=1}^Mp_i=1.</script><p>To “hint” the model, the softmax prediction probabilities are adjusted by interpolating between the original predictions $p$ and the target probability distribution $y$ during training, where the degree of interpolation is indicated by the network’s confidence $c$:</p>
<script type="math/tex; mode=display">
    p_i' = c \cdot p_i + (1-c)y_i.</script><p>The loss function $\mathcal{L}_t$ should be as usual, except for that the prediction $p$ should be the modified prediction $p’$.</p>
<p>However, if the model always predict the confidence as $0$, the loss will always be the lowest. To prevent this, <em>the confidence loss</em>, a penalty is added to the loss function. This can be interpreted as a binary cross-entropy loss, where the target value is always 1 (i.e., we want the network to always be very confident):</p>
<script type="math/tex; mode=display">
    \mathcal{L}_c=-\log(c).</script><p>So the final loss should be:</p>
<script type="math/tex; mode=display">
    \mathcal{L} = \mathcal{L}_t + \lambda \cdot \mathcal{L}_c,</script><p>where $\lambda$ is a super parameter.</p>
<h3 id="Multiple-Semantic-Label-Representations"><a href="#Multiple-Semantic-Label-Representations" class="headerlink" title="Multiple Semantic Label Representations"></a>Multiple Semantic Label Representations</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.06664">https://arxiv.org/abs/1808.06664</a></strong></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/06/21/OOD%20Detection/" data-id="cmclkr40l00036cfycuyt9yxh" data-title="OOD Detection" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine-learning</a></li></ul>

    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vision-language-model/" rel="tag">vision-language-model</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/machine-learning/" style="font-size: 20px;">machine-learning</a> <a href="/tags/vision-language-model/" style="font-size: 10px;">vision-language-model</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/01/Unsupervised%20Continual%20Domain%20Shift%20Learning%20with%20Multi-Prototype%20Modeling/">Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling</a>
          </li>
        
          <li>
            <a href="/2025/06/30/Contrastive%20Sparse%20Representation/">Contrastive Sparse Representation</a>
          </li>
        
          <li>
            <a href="/2025/06/23/Adversarial%20Pre-training%20and%20Instruction%20Tuning%20for%20Improving%20Vision-Language%20Model%20Robustness/">Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness</a>
          </li>
        
          <li>
            <a href="/2025/06/21/OOD%20Detection/">OOD Detection</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>