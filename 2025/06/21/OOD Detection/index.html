<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>OOD Detection | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Original: https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;102870562 What is OOD Detection? OOD(out-of-distribution) detection aims to detect an OOD sample, which is the opposite of ID(in-distribution) sample. In tradit">
<meta property="og:type" content="article">
<meta property="og:title" content="OOD Detection">
<meta property="og:url" content="http://example.com/2025/06/21/OOD%20Detection/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Original: https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;102870562 What is OOD Detection? OOD(out-of-distribution) detection aims to detect an OOD sample, which is the opposite of ID(in-distribution) sample. In tradit">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020250621194118.png">
<meta property="og:image" content="http://example.com/images/Pasted%20image%2020250621231213.png">
<meta property="article:published_time" content="2025-06-21T08:31:46.713Z">
<meta property="article:modified_time" content="2025-07-02T05:43:03.027Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="machine-learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/Pasted%20image%2020250621194118.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-OOD Detection" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/06/21/OOD%20Detection/" class="article-date">
  <time class="dt-published" datetime="2025-06-21T08:31:46.713Z" itemprop="datePublished">2025-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      OOD Detection
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>Original: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/102870562">https://zhuanlan.zhihu.com/p/102870562</a></strong></p>
<h1 id="What-is-OOD-Detection"><a href="#What-is-OOD-Detection" class="headerlink" title="What is OOD Detection?"></a>What is OOD Detection?</h1></blockquote>
<p>OOD(out-of-distribution) detection aims to detect an OOD sample, which is the opposite of ID(in-distribution) sample.</p>
<p>In traditional machine learning methods, the training and test data are assumed to be <strong>independent identical distribution(IID)</strong>. However, in real-world scenario, test samples are likely to be OOD, or outlier. Traditional deep learning models often consider an OOD sample as a class of ID samples with high confidence, which is not reasonable. Thus it is meaningful for models to recognize OOD samples, especially for AI security areas.</p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>Existing OOD Detection methods can be roughly divided into 4 categories:</p>
<ul>
<li>Softmax-based methods.</li>
<li>Uncertainty methods.</li>
<li>Generative model methods.</li>
<li>Classifier methods.<h2 id="Softmax-based-Methods"><a href="#Softmax-based-Methods" class="headerlink" title="Softmax-based Methods"></a>Softmax-based Methods</h2>Softmax-based methods get the softmax values of the outputs of a pre-trained model. And find out how OOD and ID samples distribute by statistical analysis. It aims to enlarge the difference between OOD and ID distributions.</li>
</ul>
<p>Softmax-based methods are simple, entirely training-free yet effective, without the need to change model structure.</p>
<h3 id="SMOOD"><a href="#SMOOD" class="headerlink" title="SMOOD"></a>SMOOD</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02136">https://arxiv.org/abs/1610.02136</a></strong></p>
</blockquote>
<p><strong>SMOOD</strong>(<strong>S</strong>oft<strong>M</strong>ax <strong>OOD</strong>) is the very first work of OOD Detection task. It proposes an OOD baseline method. Its main insights are:</p>
<ol>
<li>Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection.</li>
<li>Proposed a simple yet effective baseline method to detect whether a sample is mis-classified or out-of-distribution.</li>
</ol>
<p><img src="\images\Pasted image 20250621194118.png" alt="1"><br>The above tables show that using simply max softmax probabilities to in- or out-of-distribution samples is effective. But it can not tell if the model wrongly classifies the sample.</p>
<p><strong><em>In conclusion, SMOOD justifies the softmax output of the model differs when an in- or out-of-distribution sample is given. So they can be effectively discriminated by choosing a proper threshold.</em></strong></p>
<h3 id="ODIN"><a href="#ODIN" class="headerlink" title="ODIN"></a>ODIN</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02690">https://arxiv.org/abs/1706.02690</a></strong></p>
</blockquote>
<p>SMOOD is simple and effective. But the effect is not good enough. To achieve better results, <strong>ODIN</strong> proposes to enlarge the gap between in- and out-of-distribution softmax probabilities.</p>
<p>Based on above thoughts, it introduces two main methods:</p>
<ul>
<li>Temperature Scaling</li>
<li>Input Preprocessing</li>
</ul>
<h4 id="Temperature-Scaling"><a href="#Temperature-Scaling" class="headerlink" title="Temperature Scaling"></a>Temperature Scaling</h4><script type="math/tex; mode=display">
    p_i(x;T)=\frac{\exp(f_i(x)/T)}{\sum_{j=1}^N\exp(f_i(x)/T)}</script><p>A $T$ that is large enough makes the softmax probabilities close enough to $\frac{1}{N}$.</p>
<h4 id="Image-Preprocessing"><a href="#Image-Preprocessing" class="headerlink" title="Image Preprocessing"></a>Image Preprocessing</h4><p>ODIN preprocesss the input by adding small perturbations:</p>
<script type="math/tex; mode=display">
    \widetilde{x}=x-\epsilon\text{sign}(-\bigtriangledown_x\log(p_{\hat{y}}(x;T))</script><p>$x$ and $\epsilon$ denote the input sample and perturbation magnitude, respectively.<br>In adversarial examples(where $\widetilde{x}=x+\epsilon\text{sign}$), small perturbations are added to decrease the softmax score for the true label and force the model to make a wrong prediction. In ODIN, the goal is the opposite: it aims to increase the softmax score of any given input.<br><strong><em>Why it works?</em></strong> ID sample confidence are increased dramatically while OOD sample confidence stays the same. Thus the confidence gap between ID and OOD samples is enlarged.</p>
<h2 id="Uncertainty-Methods"><a href="#Uncertainty-Methods" class="headerlink" title="Uncertainty Methods"></a>Uncertainty Methods</h2><p>Uncertainty-based methods mainly learns the uncertainty of the model predictions. The uncertainty should be high given an OOD sample and low given an ID sample.</p>
<h3 id="Learning-Confidence-for-OOD-Detection"><a href="#Learning-Confidence-for-OOD-Detection" class="headerlink" title="Learning Confidence for OOD Detection"></a>Learning Confidence for OOD Detection</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.04865">https://arxiv.org/abs/1802.04865</a></strong></p>
</blockquote>
<p><strong><em>Motivation.</em></strong> In this paper the authors propose an idea that a network can estimate its prediction confidence and ask for “hints”.</p>
<p><img src="\images\Pasted image 20250621231213.png" alt="2"><br>In order to estimate the confidence of the prediction, a confidence estimation branch is added in parallel with the original class prediction branch, as shown in the above figure. It receives the same input as the class prediction branch and outputs the confidence.<br>The confidence branch contains one or more fully-connected layers, with the final layer outputting a single scalar $c$ between 0 and 1 (parameterized as a sigmoid).<br>The output of the prediction and confidence branch would be</p>
<script type="math/tex; mode=display">p, c = f(x, \Theta), p_i,c\in[0,1],\sum_{i=1}^Mp_i=1.</script><p>To “hint” the model, the softmax prediction probabilities are adjusted by interpolating between the original predictions $p$ and the target probability distribution $y$ during training, where the degree of interpolation is indicated by the network’s confidence $c$:</p>
<script type="math/tex; mode=display">
    p_i' = c \cdot p_i + (1-c)y_i.</script><p>The loss function $\mathcal{L}_t$ should be as usual, except for that the prediction $p$ should be the modified prediction $p’$.</p>
<p>However, if the model always predict the confidence as $0$, the loss will always be the lowest. To prevent this, <em>the confidence loss</em>, a penalty is added to the loss function. This can be interpreted as a binary cross-entropy loss, where the target value is always 1 (i.e., we want the network to always be very confident):</p>
<script type="math/tex; mode=display">
    \mathcal{L}_c=-\log(c).</script><p>So the final loss should be:</p>
<script type="math/tex; mode=display">
    \mathcal{L} = \mathcal{L}_t + \lambda \cdot \mathcal{L}_c,</script><p>where $\lambda$ is a super parameter.</p>
<h3 id="Multiple-Semantic-Label-Representations"><a href="#Multiple-Semantic-Label-Representations" class="headerlink" title="Multiple Semantic Label Representations"></a>Multiple Semantic Label Representations</h3><blockquote>
<p><strong>Paper URL: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.06664">https://arxiv.org/abs/1808.06664</a></strong></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/06/21/OOD%20Detection/" data-id="cmclkr40l00036cfycuyt9yxh" data-title="OOD Detection" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine-learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/06/23/Adversarial%20Pre-training%20and%20Instruction%20Tuning%20for%20Improving%20Vision-Language%20Model%20Robustness/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine-learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vision-language-model/" rel="tag">vision-language-model</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/machine-learning/" style="font-size: 20px;">machine-learning</a> <a href="/tags/vision-language-model/" style="font-size: 10px;">vision-language-model</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">June 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/01/Unsupervised%20Continual%20Domain%20Shift%20Learning%20with%20Multi-Prototype%20Modeling/">Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling</a>
          </li>
        
          <li>
            <a href="/2025/06/30/Contrastive%20Sparse%20Representation/">Contrastive Sparse Representation</a>
          </li>
        
          <li>
            <a href="/2025/06/23/Adversarial%20Pre-training%20and%20Instruction%20Tuning%20for%20Improving%20Vision-Language%20Model%20Robustness/">Adversarial Pre-training and Instruction Tuning for Improving Vision-Language Model Robustness</a>
          </li>
        
          <li>
            <a href="/2025/06/21/OOD%20Detection/">OOD Detection</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>